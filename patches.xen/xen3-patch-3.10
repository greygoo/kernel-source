From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: Linux: 3.10
Patch-mainline: Never, SUSE-Xen specific

 This patch contains the differences between 3.9 and 3.10.

Automatically created from "patch-3.10" by xen-port-patches.py
Acked-by: jbeulich@suse.com

--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -123,7 +123,7 @@ config X86
 	select OLD_SIGSUSPEND3 if X86_32 || IA32_EMULATION
 	select OLD_SIGACTION if X86_32
 	select COMPAT_OLD_SIGACTION if IA32_EMULATION
-	select RTC_LIB
+	select RTC_LIB if !XEN_UNPRIVILEGED_GUEST
 	select HAVE_DEBUG_STACKOVERFLOW
 	select ARCH_SUPPORTS_ATOMIC_RMW
 
--- a/arch/x86/include/asm/hypervisor.h
+++ b/arch/x86/include/asm/hypervisor.h
@@ -64,5 +64,6 @@ static inline bool hypervisor_x2apic_ava
 #endif /* _ASM_X86_HYPERVISOR_H */
 
 #ifdef HAVE_XEN_PLATFORM_COMPAT_H
+#include <asm/xen/hypervisor.h>
 #include_next <asm/hypervisor.h>
 #endif
--- a/arch/x86/include/mach-xen/asm/fixmap.h
+++ b/arch/x86/include/mach-xen/asm/fixmap.h
@@ -111,11 +111,8 @@ enum fixed_addresses {
 	FIX_LI_PCIA,	/* Lithium PCI Bridge A */
 	FIX_LI_PCIB,	/* Lithium PCI Bridge B */
 #endif
-#ifdef CONFIG_X86_F00F_BUG
-	FIX_F00F_IDT,	/* Virtual mapping for IDT */
-#endif
-#ifdef CONFIG_X86_CYCLONE_TIMER
-	FIX_CYCLONE_TIMER, /*cyclone timer register*/
+#ifndef CONFIG_X86_NO_IDT
+	FIX_RO_IDT,	/* Virtual mapping for read-only IDT */
 #endif
 #ifdef CONFIG_X86_32
 	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
--- a/arch/x86/include/mach-xen/asm/pgtable_types.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_types.h
@@ -406,7 +406,6 @@ static inline void update_page_count(int
  * as a pte too.
  */
 extern pte_t *lookup_address(unsigned long address, unsigned int *level);
-extern int __split_large_page(pte_t *kpte, unsigned long address, pte_t *pbase);
 extern phys_addr_t slow_virt_to_phys(void *__address);
 
 #endif	/* !__ASSEMBLY__ */
--- a/arch/x86/include/mach-xen/asm/processor.h
+++ b/arch/x86/include/mach-xen/asm/processor.h
@@ -94,9 +94,6 @@ struct cpuinfo_x86 {
 	char			hard_math;
 #ifndef CONFIG_XEN
 	char			rfu;
-	char			fdiv_bug;
-	char			f00f_bug;
-	char			coma_bug;
 	char			pad0;
 #endif
 #else
@@ -113,7 +110,7 @@ struct cpuinfo_x86 {
 	__u32			extended_cpuid_level;
 	/* Maximum supported CPUID level, -1=no CPUID: */
 	int			cpuid_level;
-	__u32			x86_capability[NCAPINTS];
+	__u32			x86_capability[NCAPINTS + NBUGINTS];
 	char			x86_vendor_id[16];
 	char			x86_model_id[64];
 	/* in KB - valid for CPUS which support this call: */
@@ -972,26 +969,6 @@ unsigned long calc_aperfmperf_ratio(stru
 }
 #endif
 
-/*
- * AMD errata checking
- */
-#ifdef CONFIG_CPU_SUP_AMD
-extern const int amd_erratum_383[];
-extern const int amd_erratum_400[];
-extern bool cpu_has_amd_erratum(const int *);
-
-#define AMD_LEGACY_ERRATUM(...)		{ -1, __VA_ARGS__, 0 }
-#define AMD_OSVW_ERRATUM(osvw_id, ...)	{ osvw_id, __VA_ARGS__, 0 }
-#define AMD_MODEL_RANGE(f, m_start, s_start, m_end, s_end) \
-	((f << 24) | (m_start << 16) | (s_start << 12) | (m_end << 4) | (s_end))
-#define AMD_MODEL_RANGE_FAMILY(range)	(((range) >> 24) & 0xff)
-#define AMD_MODEL_RANGE_START(range)	(((range) >> 12) & 0xfff)
-#define AMD_MODEL_RANGE_END(range)	((range) & 0xfff)
-
-#else
-#define cpu_has_amd_erratum(x)	(false)
-#endif /* CONFIG_CPU_SUP_AMD */
-
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 
--- a/arch/x86/kernel/cpu/Makefile
+++ b/arch/x86/kernel/cpu/Makefile
@@ -50,8 +50,7 @@ obj-$(CONFIG_X86_LOCAL_APIC)		+= perfctr
 
 obj-$(CONFIG_HYPERVISOR_GUEST)		+= vmware.o hypervisor.o mshyperv.o
 
-disabled-obj-$(CONFIG_XEN) := hypervisor.o mshyperv.o perfctr-watchdog.o \
-			      perf_event.o perf_event_%.o sched.o vmware.o
+disabled-obj-$(CONFIG_XEN) := perfctr-watchdog.o perf_event.o perf_event_%.o sched.o
 
 quiet_cmd_mkcapflags = MKCAP   $@
       cmd_mkcapflags = $(CONFIG_SHELL) $(srctree)/$(src)/mkcapflags.sh $< $@
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -531,9 +531,11 @@ static void early_init_amd(struct cpuinf
 
 }
 
+#ifndef CONFIG_XEN
 static const int amd_erratum_383[];
 static const int amd_erratum_400[];
 static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum);
+#endif
 
 static void init_amd(struct cpuinfo_x86 *c)
 {
@@ -872,6 +874,7 @@ static const struct cpu_dev amd_cpu_dev
 
 cpu_dev_register(amd_cpu_dev);
 
+#ifndef CONFIG_XEN
 /*
  * AMD errata checking
  *
@@ -935,3 +938,4 @@ static bool cpu_has_amd_erratum(struct c
 
 	return false;
 }
+#endif
--- a/arch/x86/kernel/cpu/common-xen.c
+++ b/arch/x86/kernel/cpu/common-xen.c
@@ -975,6 +975,10 @@ static void __cpuinit identify_cpu(struc
 		/* AND the already accumulated flags with these */
 		for (i = 0; i < NCAPINTS; i++)
 			boot_cpu_data.x86_capability[i] &= c->x86_capability[i];
+
+		/* OR, i.e. replicate the bug flags */
+		for (i = NCAPINTS; i < NCAPINTS + NBUGINTS; i++)
+			c->x86_capability[i] |= boot_cpu_data.x86_capability[i];
 	}
 
 	/* Init Machine Check Exception if available. */
--- a/arch/x86/kernel/early_printk-xen.c
+++ b/arch/x86/kernel/early_printk-xen.c
@@ -197,25 +197,9 @@ static struct console early_serial_conso
 	.index =	-1,
 };
 
-/* Direct interface for emergencies */
-static struct console *early_console = &early_vga_console;
-static int __initdata early_console_initialized;
-
-asmlinkage void early_printk(const char *fmt, ...)
-{
-	char buf[512];
-	int n;
-	va_list ap;
-
-	va_start(ap, fmt);
-	n = vscnprintf(buf, sizeof(buf), fmt, ap);
-	early_console->write(early_console, buf, n);
-	va_end(ap);
-}
-
 static inline void early_console_register(struct console *con, int keep_early)
 {
-	if (early_console->index != -1) {
+	if (con->index != -1) {
 		printk(KERN_CRIT "ERROR: earlyprintk= %s already used\n",
 		       con->name);
 		return;
@@ -235,9 +219,8 @@ static int __init setup_early_printk(cha
 	if (!buf)
 		return 0;
 
-	if (early_console_initialized)
+	if (early_console)
 		return 0;
-	early_console_initialized = 1;
 
 	keep = (strstr(buf, "keep") != NULL);
 
--- a/arch/x86/kernel/head64-xen.c
+++ b/arch/x86/kernel/head64-xen.c
@@ -36,6 +36,7 @@
 extern pgd_t early_level4_pgt[PTRS_PER_PGD];
 extern pmd_t early_dynamic_pgts[EARLY_DYNAMIC_PAGE_TABLES][PTRS_PER_PMD];
 static unsigned int __initdata next_early_pgt = 2;
+pmdval_t early_pmd_flags = __PAGE_KERNEL_LARGE & ~(_PAGE_GLOBAL | _PAGE_NX);
 
 /* Wipe all early page tables except for the kernel symbol map */
 static void __init reset_early_page_tables(void)
@@ -101,7 +102,7 @@ again:
 			pmd_p[i] = 0;
 		*pud_p = (pudval_t)pmd_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;
 	}
-	pmd = (physaddr & PMD_MASK) + (__PAGE_KERNEL_LARGE & ~_PAGE_GLOBAL);
+	pmd = (physaddr & PMD_MASK) + early_pmd_flags;
 	pmd_p[pmd_index(address)] = pmd;
 
 	return 0;
@@ -158,10 +159,10 @@ void __init x86_64_start_kernel(char * r
 	 * Build-time sanity checks on the kernel image and module
 	 * area mappings. (these are purely build-time and produce no code)
 	 */
-	BUILD_BUG_ON(MODULES_VADDR < KERNEL_IMAGE_START);
-	BUILD_BUG_ON(MODULES_VADDR-KERNEL_IMAGE_START < KERNEL_IMAGE_SIZE);
+	BUILD_BUG_ON(MODULES_VADDR < __START_KERNEL_map);
+	BUILD_BUG_ON(MODULES_VADDR - __START_KERNEL_map < KERNEL_IMAGE_SIZE);
 	BUILD_BUG_ON(MODULES_LEN + KERNEL_IMAGE_SIZE > 2*PUD_SIZE);
-	BUILD_BUG_ON((KERNEL_IMAGE_START & ~PMD_MASK) != 0);
+	BUILD_BUG_ON((__START_KERNEL_map & ~PMD_MASK) != 0);
 	BUILD_BUG_ON((MODULES_VADDR & ~PMD_MASK) != 0);
 	BUILD_BUG_ON(!(MODULES_VADDR > __START_KERNEL));
 	BUILD_BUG_ON(!(((MODULES_END - 1) & PGDIR_MASK) ==
--- a/arch/x86/kernel/irq-xen.c
+++ b/arch/x86/kernel/irq-xen.c
@@ -184,10 +184,6 @@ u64 arch_irq_stat_cpu(unsigned int cpu)
 u64 arch_irq_stat(void)
 {
 	u64 sum = atomic_read(&irq_err_count);
-
-#ifdef CONFIG_X86_IO_APIC
-	sum += atomic_read(&irq_mis_count);
-#endif
 	return sum;
 }
 
@@ -247,6 +243,28 @@ void smp_x86_platform_ipi(struct pt_regs
 	set_irq_regs(old_regs);
 }
 
+#ifdef CONFIG_HAVE_KVM
+/*
+ * Handler for POSTED_INTERRUPT_VECTOR.
+ */
+void smp_kvm_posted_intr_ipi(struct pt_regs *regs)
+{
+	struct pt_regs *old_regs = set_irq_regs(regs);
+
+	ack_APIC_irq();
+
+	irq_enter();
+
+	exit_idle();
+
+	inc_irq_stat(kvm_posted_intr_ipis);
+
+	irq_exit();
+
+	set_irq_regs(old_regs);
+}
+#endif
+
 EXPORT_SYMBOL_GPL(vector_used_by_percpu_irq);
 #endif
 
--- a/arch/x86/kernel/process-xen.c
+++ b/arch/x86/kernel/process-xen.c
@@ -125,30 +125,6 @@ void exit_thread(void)
 	drop_fpu(me);
 }
 
-void show_regs_common(void)
-{
-	const char *vendor, *product, *board;
-
-	vendor = dmi_get_system_info(DMI_SYS_VENDOR);
-	if (!vendor)
-		vendor = "";
-	product = dmi_get_system_info(DMI_PRODUCT_NAME);
-	if (!product)
-		product = "";
-
-	/* Board Name is optional */
-	board = dmi_get_system_info(DMI_BOARD_NAME);
-
-	printk(KERN_DEFAULT "Pid: %d, comm: %.20s %s %s %.*s %s %s%s%s\n",
-	       current->pid, current->comm, print_tainted(),
-	       init_utsname()->release,
-	       (int)strcspn(init_utsname()->version, " "),
-	       init_utsname()->version,
-	       vendor, product,
-	       board ? "/" : "",
-	       board ? board : "");
-}
-
 void flush_thread(void)
 {
 	struct task_struct *tsk = current;
@@ -257,7 +233,9 @@ void __switch_to_xtra(struct task_struct
 unsigned long boot_option_idle_override = IDLE_NO_OVERRIDE;
 EXPORT_SYMBOL(boot_option_idle_override);
 
+#ifndef CONFIG_XEN
 static void (*x86_idle)(void);
+#endif
 
 #ifndef CONFIG_SMP
 static inline void play_dead(void)
@@ -290,87 +268,40 @@ void exit_idle(void)
 }
 #endif
 
-/*
- * The idle thread. There's no useful work to be
- * done, so just try to conserve power and have a
- * low exit latency (ie sit in a loop waiting for
- * somebody to say that they'd like to reschedule)
- */
-void cpu_idle(void)
+void arch_cpu_idle_enter(void)
 {
-	/*
-	 * If we're the non-boot CPU, nothing set the stack canary up
-	 * for us.  CPU0 already has it initialized but no harm in
-	 * doing it again.  This is a good place for updating it, as
-	 * we wont ever return from this function (so the invalid
-	 * canaries already on the stack wont ever trigger).
-	 */
-	boot_init_stack_canary();
-	current_thread_info()->status |= TS_POLLING;
-
-	while (1) {
-		tick_nohz_idle_enter();
-
-		while (!need_resched()) {
-			rmb();
-
-			if (cpu_is_offline(smp_processor_id()))
-				play_dead();
-
-			/*
-			 * Idle routines should keep interrupts disabled
-			 * from here on, until they go to idle.
-			 * Otherwise, idle callbacks can misfire.
-			 */
-			local_touch_nmi();
-			local_irq_disable();
-
-			enter_idle();
-
-			/* Don't trace irqs off for idle */
-			stop_critical_timings();
-
-			/* enter_idle() needs rcu for notifiers */
-			rcu_idle_enter();
-
-			if (cpuidle_idle_call())
-				xen_idle();
+	local_touch_nmi();
+	enter_idle();
+}
 
-			rcu_idle_exit();
-			start_critical_timings();
+void arch_cpu_idle_exit(void)
+{
+	__exit_idle();
+}
 
-			/* In many cases the interrupt that ended idle
-			   has already called exit_idle. But some idle
-			   loops can be woken up without interrupt. */
-			__exit_idle();
-		}
+void arch_cpu_idle_dead(void)
+{
+	play_dead();
+}
 
-		tick_nohz_idle_exit();
-		preempt_enable_no_resched();
-		schedule();
-		preempt_disable();
-	}
+/*
+ * Called from the generic idle code.
+ */
+void arch_cpu_idle(void)
+{
+	if (cpuidle_idle_call())
+		xen_idle();
+	else
+		local_irq_enable();
 }
 
 /*
- * We use this if we don't have any better
- * idle routine..
+ * We use this if we don't have any better idle routine..
  */
 void xen_idle(void)
 {
 	trace_cpu_idle_rcuidle(1, smp_processor_id());
-	current_thread_info()->status &= ~TS_POLLING;
-	/*
-	 * TS_POLLING-cleared state must be visible before we
-	 * test NEED_RESCHED:
-	 */
-	smp_mb();
-
-	if (!need_resched())
-		safe_halt();	/* enables interrupts racelessly */
-	else
-		local_irq_enable();
-	current_thread_info()->status |= TS_POLLING;
+	safe_halt();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 #ifdef CONFIG_APM_MODULE
@@ -400,20 +331,6 @@ void stop_this_cpu(void *dummy)
 		halt();
 }
 
-/*
- * On SMP it's slightly faster (but much more power-consuming!)
- * to poll the ->work.need_resched flag instead of waiting for the
- * cross-CPU IPI to arrive. Use this option with caution.
- */
-static void poll_idle(void)
-{
-	trace_cpu_idle_rcuidle(0, smp_processor_id());
-	local_irq_enable();
-	while (!need_resched())
-		cpu_relax();
-	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
-}
-
 #ifndef CONFIG_XEN
 bool amd_e400_c1e_detected;
 EXPORT_SYMBOL(amd_e400_c1e_detected);
@@ -433,9 +350,6 @@ void amd_e400_remove_cpu(int cpu)
  */
 static void amd_e400_idle(void)
 {
-	if (need_resched())
-		return;
-
 	if (!amd_e400_c1e_detected) {
 		u32 lo, hi;
 
@@ -481,13 +395,13 @@ void __cpuinit select_idle_routine(const
 {
 #ifndef CONFIG_XEN
 #ifdef CONFIG_SMP
-	if (x86_idle == poll_idle && smp_num_siblings > 1)
+	if (boot_option_idle_override == IDLE_POLL && smp_num_siblings > 1)
 		pr_warn_once("WARNING: polling idle and HT enabled, performance may degrade\n");
 #endif
-	if (x86_idle)
+	if (x86_idle || boot_option_idle_override == IDLE_POLL)
 		return;
 
-	if (cpu_has_amd_erratum(amd_erratum_400)) {
+	if (cpu_has_bug(c, X86_BUG_AMD_APIC_C1E)) {
 		/* E400: APIC timer interrupt does not wake up CPU from C1e */
 		pr_info("using AMD E400 aware idle routine\n");
 		x86_idle = amd_e400_idle;
@@ -512,8 +426,8 @@ static int __init idle_setup(char *str)
 
 	if (!strcmp(str, "poll")) {
 		pr_info("using polling idle threads\n");
-		x86_idle = poll_idle;
 		boot_option_idle_override = IDLE_POLL;
+		cpu_idle_poll_ctrl(true);
 #ifndef CONFIG_XEN
 	} else if (!strcmp(str, "halt")) {
 		/*
--- a/arch/x86/kernel/process_64-xen.c
+++ b/arch/x86/kernel/process_64-xen.c
@@ -65,7 +65,6 @@ void __show_regs(struct pt_regs *regs, i
 	unsigned int fsindex, gsindex;
 	unsigned int ds, cs, es;
 
-	show_regs_common();
 	printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
 	printk_address(regs->ip, 1);
 	printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
--- a/arch/x86/kernel/setup-xen.c
+++ b/arch/x86/kernel/setup-xen.c
@@ -81,7 +81,6 @@
 #include <asm/timer.h>
 #include <asm/i8259.h>
 #include <asm/sections.h>
-#include <asm/dmi.h>
 #include <asm/io_apic.h>
 #include <asm/ist.h>
 #include <asm/setup_arch.h>
@@ -1119,8 +1118,18 @@ void __init setup_arch(char **cmdline_p)
 	if (efi_enabled(EFI_BOOT))
 		efi_init();
 
-	if (is_initial_xendomain())
+	if (is_initial_xendomain()) {
 		dmi_scan_machine();
+		dmi_set_dump_stack_arch_desc();
+	} else {
+		int ver = HYPERVISOR_xen_version(XENVER_version, NULL);
+		xen_extraversion_t extra;
+
+		if (HYPERVISOR_xen_version(XENVER_extraversion, extra))
+			*extra = 0;
+		dump_stack_set_arch_desc("Xen %d.%d%s PV guest",
+					 ver >> 16, ver & 0xff, extra);
+	}
 
 	/*
 	 * VMware detection requires dmi to be available, so this
--- a/arch/x86/kernel/traps-xen.c
+++ b/arch/x86/kernel/traps-xen.c
@@ -12,6 +12,7 @@
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
+#include <linux/context_tracking.h>
 #include <linux/interrupt.h>
 #include <linux/kallsyms.h>
 #include <linux/spinlock.h>
@@ -55,8 +56,7 @@
 #include <asm/i387.h>
 #include <asm/fpu-internal.h>
 #include <asm/mce.h>
-#include <asm/context_tracking.h>
-
+#include <asm/fixmap.h>
 #include <asm/mach_traps.h>
 
 #ifdef CONFIG_X86_64
@@ -180,34 +180,38 @@ do_trap(int trapnr, int signr, char *str
 #define DO_ERROR(trapnr, signr, str, name)				\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
-	exception_enter(regs);						\
+	enum ctx_state prev_state;					\
+									\
+	prev_state = exception_enter();					\
 	if (notify_die(DIE_TRAP, str, regs, error_code,			\
 			trapnr, signr) == NOTIFY_STOP) {		\
-		exception_exit(regs);					\
+		exception_exit(prev_state);				\
 		return;							\
 	}								\
 	conditional_sti(regs);						\
 	do_trap(trapnr, signr, str, regs, error_code, NULL);		\
-	exception_exit(regs);						\
+	exception_exit(prev_state);					\
 }
 
 #define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr)		\
 dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 {									\
 	siginfo_t info;							\
+	enum ctx_state prev_state;					\
+									\
 	info.si_signo = signr;						\
 	info.si_errno = 0;						\
 	info.si_code = sicode;						\
 	info.si_addr = (void __user *)siaddr;				\
-	exception_enter(regs);						\
+	prev_state = exception_enter();					\
 	if (notify_die(DIE_TRAP, str, regs, error_code,			\
 			trapnr, signr) == NOTIFY_STOP) {		\
-		exception_exit(regs);					\
+		exception_exit(prev_state);				\
 		return;							\
 	}								\
 	conditional_sti(regs);						\
 	do_trap(trapnr, signr, str, regs, error_code, &info);		\
-	exception_exit(regs);						\
+	exception_exit(prev_state);					\
 }
 
 DO_ERROR_INFO(X86_TRAP_DE, SIGFPE, "divide error", divide_error, FPE_INTDIV,
@@ -230,14 +234,16 @@ DO_ERROR_INFO(X86_TRAP_AC, SIGBUS, "alig
 /* Runs on IST stack */
 dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	if (notify_die(DIE_TRAP, "stack segment", regs, error_code,
 		       X86_TRAP_SS, SIGBUS) != NOTIFY_STOP) {
 		preempt_conditional_sti(regs);
 		do_trap(X86_TRAP_SS, SIGBUS, "stack segment", regs, error_code, NULL);
 		preempt_conditional_cli(regs);
 	}
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
@@ -245,7 +251,7 @@ dotraplinkage void do_double_fault(struc
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
 
-	exception_enter(regs);
+	exception_enter();
 	/* Return not checked because double check cannot be ignored */
 	notify_die(DIE_TRAP, str, regs, error_code, X86_TRAP_DF, SIGSEGV);
 
@@ -265,8 +271,9 @@ dotraplinkage void __kprobes
 do_general_protection(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk;
+	enum ctx_state prev_state;
 
-	exception_enter(regs);
+	prev_state = exception_enter();
 	conditional_sti(regs);
 
 #ifdef CONFIG_X86_32
@@ -304,12 +311,14 @@ do_general_protection(struct pt_regs *re
 
 	force_sig(SIGSEGV, tsk);
 exit:
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 /* May run on IST stack. */
 dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_code)
 {
+	enum ctx_state prev_state;
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 	/*
 	 * ftrace must be first, everything else may cause a recursive crash.
@@ -319,7 +328,7 @@ dotraplinkage void __kprobes notrace do_
 	    ftrace_int3_handler(regs))
 		return;
 #endif
-	exception_enter(regs);
+	prev_state = exception_enter();
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -340,7 +349,7 @@ dotraplinkage void __kprobes notrace do_
 	preempt_conditional_cli(regs);
 	debug_stack_usage_dec();
 exit:
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 #if defined(CONFIG_X86_64) && !defined(CONFIG_XEN)
@@ -397,11 +406,12 @@ asmlinkage __kprobes struct pt_regs *syn
 dotraplinkage void __kprobes do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
+	enum ctx_state prev_state;
 	int user_icebp = 0;
 	unsigned long dr6;
 	int si_code;
 
-	exception_enter(regs);
+	prev_state = exception_enter();
 
 	get_debugreg(dr6, 6);
 
@@ -471,7 +481,7 @@ dotraplinkage void __kprobes do_debug(st
 	debug_stack_usage_dec();
 
 exit:
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 /*
@@ -565,17 +575,21 @@ void math_error(struct pt_regs *regs, in
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	math_error(regs, error_code, X86_TRAP_MF);
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 dotraplinkage void
 do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	math_error(regs, error_code, X86_TRAP_XF);
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 #ifndef CONFIG_XEN
@@ -652,7 +666,9 @@ void math_state_restore(void)
 dotraplinkage void __kprobes
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	BUG_ON(use_eager_fpu());
 
 #ifdef CONFIG_MATH_EMULATION
@@ -663,7 +679,7 @@ do_device_not_available(struct pt_regs *
 
 		info.regs = regs;
 		math_emulate(&info);
-		exception_exit(regs);
+		exception_exit(prev_state);
 		return;
 	}
 #endif
@@ -673,15 +689,16 @@ do_device_not_available(struct pt_regs *
 #ifdef CONFIG_X86_32
 	conditional_sti(regs);
 #endif
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 
 #ifdef CONFIG_X86_32
 dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code)
 {
 	siginfo_t info;
+	enum ctx_state prev_state;
 
-	exception_enter(regs);
+	prev_state = exception_enter();
 	local_irq_enable();
 
 	info.si_signo = SIGILL;
@@ -693,7 +710,7 @@ dotraplinkage void do_iret_error(struct
 		do_trap(X86_TRAP_IRET, SIGILL, "iret exception", regs, error_code,
 			&info);
 	}
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
 #endif
 
--- a/arch/x86/mm/fault-xen.c
+++ b/arch/x86/mm/fault-xen.c
@@ -13,12 +13,12 @@
 #include <linux/perf_event.h>		/* perf_sw_event		*/
 #include <linux/hugetlb.h>		/* hstate_index_to_shift	*/
 #include <linux/prefetch.h>		/* prefetchw			*/
+#include <linux/context_tracking.h>	/* exception_enter(), ...	*/
 
 #include <asm/traps.h>			/* dotraplinkage, ...		*/
 #include <asm/pgalloc.h>		/* pgd_*(), ...			*/
 #include <asm/kmemcheck.h>		/* kmemcheck_*(), ...		*/
 #include <asm/fixmap.h>			/* VSYSCALL_START		*/
-#include <asm/context_tracking.h>	/* exception_enter(), ...	*/
 
 /*
  * Page fault error code bits:
@@ -566,7 +566,7 @@ static int is_f00f_bug(struct pt_regs *r
 	/*
 	 * Pentium F0 0F C7 C8 bug workaround:
 	 */
-	if (boot_cpu_data.f00f_bug) {
+	if (boot_cpu_has_bug(X86_BUG_F00F)) {
 		nr = (address - idt_descr.address) >> 3;
 
 		if (nr == 6) {
@@ -1253,7 +1253,9 @@ good_area:
 dotraplinkage void __kprobes
 do_page_fault(struct pt_regs *regs, unsigned long error_code)
 {
-	exception_enter(regs);
+	enum ctx_state prev_state;
+
+	prev_state = exception_enter();
 	__do_page_fault(regs, error_code);
-	exception_exit(regs);
+	exception_exit(prev_state);
 }
--- a/arch/x86/mm/init-xen.c
+++ b/arch/x86/mm/init-xen.c
@@ -294,6 +294,9 @@ static int __meminit split_mem_range(str
 	end_pfn = limit_pfn;
 	nr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);
 
+	if (!after_bootmem)
+		adjust_range_page_size_mask(mr, nr_range);
+
 	/* try to merge same page size and continuous */
 	for (i = 0; nr_range > 1 && i < nr_range - 1; i++) {
 		unsigned long old_start;
@@ -308,9 +311,6 @@ static int __meminit split_mem_range(str
 		nr_range--;
 	}
 
-	if (!after_bootmem)
-		adjust_range_page_size_mask(mr, nr_range);
-
 	for (i = 0; i < nr_range; i++)
 		printk(KERN_DEBUG " [mem %#010lx-%#010lx] page %s\n",
 				mr[i].start, mr[i].end - 1,
@@ -376,7 +376,17 @@ unsigned long __init_refok init_memory_m
 }
 
 /*
- * would have hole in the middle or ends, and only ram parts will be mapped.
+ * We need to iterate through the E820 memory map and create direct mappings
+ * for only E820_RAM and E820_KERN_RESERVED regions. We cannot simply
+ * create direct mappings for all pfns from [0 to max_low_pfn) and
+ * [4GB to max_pfn) because of possible memory holes in high addresses
+ * that cannot be marked as UC by fixed/variable range MTRRs.
+ * Depending on the alignment of E820 ranges, this may possibly result
+ * in using smaller size (i.e. 4K instead of 2M or 1G) page tables.
+ *
+ * init_mem_mapping() calls init_range_memory_mapping() with big range.
+ * That range would have hole in the middle or ends, and only ram parts
+ * will be mapped in init_range_memory_mapping().
  */
 static unsigned long __init init_range_memory_mapping(
 					   unsigned long r_start,
@@ -433,6 +443,13 @@ void __init init_mem_mapping(void)
 	max_pfn_mapped = 0; /* will get exact value next */
 	min_pfn_mapped = real_end >> PAGE_SHIFT;
 	last_start = start = real_end;
+
+	/*
+	 * We start from the top (end of memory) and go to the bottom.
+	 * The memblock_find_in_range() gets us a block of RAM from the
+	 * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages
+	 * for page table.
+	 */
 	while (last_start) {
 		start = round_down(last_start - 1, step_size);
 		new_mapped_ram_size = init_range_memory_mapping(start,
@@ -525,8 +542,6 @@ void free_init_pages(char *what, unsigne
 	printk(KERN_INFO "Freeing %s: %luk freed\n", what, (end - begin) >> 10);
 
 	for (; addr < end; addr += PAGE_SIZE) {
-		ClearPageReserved(virt_to_page(addr));
-		init_page_count(virt_to_page(addr));
 		memset((void *)addr, POISON_FREE_INITMEM, PAGE_SIZE);
 #ifdef CONFIG_X86_64
 		if (addr >= __START_KERNEL_map) {
@@ -542,8 +557,7 @@ void free_init_pages(char *what, unsigne
 				BUG();
 		}
 #endif
-		free_page(addr);
-		totalram_pages++;
+		free_reserved_page(virt_to_page(addr));
 	}
 #endif
 }
--- a/arch/x86/mm/init_64-xen.c
+++ b/arch/x86/mm/init_64-xen.c
@@ -35,6 +35,7 @@
 #include <linux/memory_hotplug.h>
 #include <linux/nmi.h>
 #include <linux/gfp.h>
+#include <linux/kcore.h>
 
 #include <asm/processor.h>
 #include <asm/bios_ebda.h>
@@ -1326,14 +1327,12 @@ remove_pagetable(unsigned long start, un
 	flush_tlb_all();
 }
 
-void __ref vmemmap_free(struct page *memmap, unsigned long nr_pages)
+void __ref vmemmap_free(unsigned long start, unsigned long end)
 {
-	unsigned long start = (unsigned long)memmap;
-	unsigned long end = (unsigned long)(memmap + nr_pages);
-
 	remove_pagetable(start, end, false);
 }
 
+#ifdef CONFIG_MEMORY_HOTREMOVE
 static void __meminit
 kernel_physical_mapping_remove(unsigned long start, unsigned long end)
 {
@@ -1343,7 +1342,6 @@ kernel_physical_mapping_remove(unsigned
 	remove_pagetable(start, end, true);
 }
 
-#ifdef CONFIG_MEMORY_HOTREMOVE
 int __ref arch_remove_memory(u64 start, u64 size)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
@@ -1383,10 +1381,9 @@ void __init mem_init(void)
 
 	/* clear_bss() already clear the empty_zero_page */
 
-	reservedpages = 0;
-
-	/* this will put all low memory onto the freelists */
 	register_page_bootmem_info();
+
+	/* this will put all memory onto the freelists */
 	totalram_pages = free_all_bootmem();
 
 	/* XEN: init pages outside initial allocation. */
@@ -1620,18 +1617,17 @@ static long __meminitdata addr_start, ad
 static void __meminitdata *p_start, *p_end;
 static int __meminitdata node_start;
 
-int __meminit
-vmemmap_populate(struct page *start_page, unsigned long size, int node)
+static int __meminit vmemmap_populate_hugepages(unsigned long start,
+						unsigned long end, int node)
 {
-	unsigned long addr = (unsigned long)start_page;
-	unsigned long end = (unsigned long)(start_page + size);
+	unsigned long addr;
 	unsigned long next;
 	pgd_t *pgd;
 	pud_t *pud;
 	pmd_t *pmd;
 
-	for (; addr < end; addr = next) {
-		void *p = NULL;
+	for (addr = start; addr < end; addr = next) {
+		next = pmd_addr_end(addr, end);
 
 		pgd = vmemmap_pgd_populate(addr, node);
 		if (!pgd)
@@ -1641,31 +1637,14 @@ vmemmap_populate(struct page *start_page
 		if (!pud)
 			return -ENOMEM;
 
-		if (!cpu_has_pse) {
-			next = (addr + PAGE_SIZE) & PAGE_MASK;
-			pmd = vmemmap_pmd_populate(pud, addr, node);
-
-			if (!pmd)
-				return -ENOMEM;
-
-			p = vmemmap_pte_populate(pmd, addr, node);
-
-			if (!p)
-				return -ENOMEM;
-
-			addr_end = addr + PAGE_SIZE;
-			p_end = p + PAGE_SIZE;
-		} else {
-			next = pmd_addr_end(addr, end);
+		pmd = pmd_offset(pud, addr);
+		if (pmd_none(*pmd)) {
+			void *p;
 
-			pmd = pmd_offset(pud, addr);
-			if (pmd_none(*pmd)) {
+			p = vmemmap_alloc_block_buf(PMD_SIZE, node);
+			if (p) {
 				pte_t entry;
 
-				p = vmemmap_alloc_block_buf(PMD_SIZE, node);
-				if (!p)
-					return -ENOMEM;
-
 				entry = pfn_pte(__pa(p) >> PAGE_SHIFT,
 						PAGE_KERNEL_LARGE);
 				set_pmd(pmd, __pmd_ma(__pte_val(entry)));
@@ -1682,15 +1661,32 @@ vmemmap_populate(struct page *start_page
 
 				addr_end = addr + PMD_SIZE;
 				p_end = p + PMD_SIZE;
-			} else
-				vmemmap_verify((pte_t *)pmd, node, addr, next);
+				continue;
+			}
+		} else if (pmd_large(*pmd)) {
+			vmemmap_verify((pte_t *)pmd, node, addr, next);
+			continue;
 		}
-
+		pr_warn_once("vmemmap: falling back to regular page backing\n");
+		if (vmemmap_populate_basepages(addr, next, node))
+			return -ENOMEM;
 	}
-	sync_global_pgds((unsigned long)start_page, end - 1);
 	return 0;
 }
 
+int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)
+{
+	int err;
+
+	if (cpu_has_pse)
+		err = vmemmap_populate_hugepages(start, end, node);
+	else
+		err = vmemmap_populate_basepages(start, end, node);
+	if (!err)
+		sync_global_pgds(start, end - 1);
+	return err;
+}
+
 #if defined(CONFIG_MEMORY_HOTPLUG_SPARSE) && defined(CONFIG_HAVE_BOOTMEM_INFO_NODE)
 void register_page_bootmem_memmap(unsigned long section_nr,
 				  struct page *start_page, unsigned long size)
--- a/arch/x86/mm/ioremap-xen.c
+++ b/arch/x86/mm/ioremap-xen.c
@@ -448,12 +448,7 @@ void iounmap(volatile void __iomem *addr
 	   in parallel. Reuse of the virtual address is prevented by
 	   leaving it in the global lists until we're done with it.
 	   cpa takes care of the direct mappings. */
-	read_lock(&vmlist_lock);
-	for (p = vmlist; p; p = p->next) {
-		if (p->addr == (void __force *)addr)
-			break;
-	}
-	read_unlock(&vmlist_lock);
+	p = find_vm_area((void __force *)addr);
 
 	if (!p) {
 		printk(KERN_ERR "iounmap: bad address %p\n", addr);
--- a/arch/x86/mm/pageattr-xen.c
+++ b/arch/x86/mm/pageattr-xen.c
@@ -555,13 +555,14 @@ out_unlock:
 	return do_split;
 }
 
-int __split_large_page(pte_t *kpte, unsigned long address, pte_t *pbase)
+static int
+__split_large_page(pte_t *kpte, unsigned long address, struct page *base)
 {
+	pte_t *pbase = (pte_t *)page_address(base);
 	unsigned long mfn, mfninc = 1;
 	unsigned int i, level;
 	pte_t *tmp;
 	pgprot_t ref_prot;
-	struct page *base = virt_to_page(pbase);
 
 	spin_lock(&pgd_lock);
 	/*
@@ -650,7 +651,6 @@ int __split_large_page(pte_t *kpte, unsi
 
 static int split_large_page(pte_t *kpte, unsigned long address)
 {
-	pte_t *pbase;
 	struct page *base;
 
 	if (!debug_pagealloc)
@@ -661,8 +661,7 @@ static int split_large_page(pte_t *kpte,
 	if (!base)
 		return -ENOMEM;
 
-	pbase = (pte_t *)page_address(base);
-	if (__split_large_page(kpte, address, pbase))
+	if (__split_large_page(kpte, address, base))
 		__free_page(base);
 
 	return 0;
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -1696,7 +1696,7 @@ void __ref xen_hvm_init_shared_info(void
 	 * in that case multiple vcpus might be online. */
 	for_each_online_cpu(cpu) {
 		/* Leave it to be NULL. */
-		if (cpu >= MAX_VIRT_CPUS)
+		if (cpu >= XEN_LEGACY_MAX_VCPUS)
 			continue;
 		per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[cpu];
 	}
--- a/drivers/gpu/drm/i915/intel_display.c
+++ b/drivers/gpu/drm/i915/intel_display.c
@@ -1826,7 +1826,7 @@ static void intel_disable_plane(struct d
 
 static bool need_vtd_wa(struct drm_device *dev)
 {
-#ifdef CONFIG_INTEL_IOMMU
+#if defined(CONFIG_INTEL_IOMMU) || defined(CONFIG_XEN)
 	if (INTEL_INFO(dev)->gen >= 6 && intel_iommu_gfx_mapped)
 		return true;
 #endif
--- a/drivers/hwmon/coretemp-xen.c
+++ b/drivers/hwmon/coretemp-xen.c
@@ -414,8 +414,7 @@ static int chk_ucode_version(unsigned in
 	 * fixed for stepping D0 (6EC).
 	 */
 	if (c->x86_model == 0xe && c->x86_mask < 0xc && c->microcode < 0x39) {
-		pr_err("Errata AE18 not fixed, update BIOS or "
-		       "microcode of the CPU!\n");
+		pr_err("Errata AE18 not fixed, update BIOS or microcode of the CPU!\n");
 		return -ENODEV;
 	}
 	return 0;
--- a/drivers/net/caif/Kconfig
+++ b/drivers/net/caif/Kconfig
@@ -43,7 +43,7 @@ config CAIF_HSI
 
 config CAIF_VIRTIO
 	tristate "CAIF virtio transport driver"
-	depends on CAIF && HAS_DMA
+	depends on CAIF && !XEN && HAS_DMA
 	select VHOST_RING
 	select VIRTIO
 	select GENERIC_ALLOCATOR
--- a/drivers/pci/msi-xen.c
+++ b/drivers/pci/msi-xen.c
@@ -26,7 +26,6 @@
 #include <xen/pcifront.h>
 
 #include "pci.h"
-#include "msi.h"
 
 static bool pci_msi_enable = true;
 #if CONFIG_XEN_COMPAT < 0x040200
@@ -56,6 +55,9 @@ struct msi_dev_list {
 	struct msi_pirq_entry e;
 };
 
+#define msix_table_size(flags)	((flags & PCI_MSIX_FLAGS_QSIZE) + 1)
+
+
 /* Arch hooks */
 
 #ifndef arch_msi_check_device
@@ -65,32 +67,26 @@ int arch_msi_check_device(struct pci_dev
 }
 #endif
 
-static void msi_set_enable(struct pci_dev *dev, int pos, int enable)
+static void msi_set_enable(struct pci_dev *dev, int enable)
 {
 	u16 control;
 
-	BUG_ON(!pos);
-
-	pci_read_config_word(dev, pos + PCI_MSI_FLAGS, &control);
+	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);
 	control &= ~PCI_MSI_FLAGS_ENABLE;
 	if (enable)
 		control |= PCI_MSI_FLAGS_ENABLE;
-	pci_write_config_word(dev, pos + PCI_MSI_FLAGS, control);
+	pci_write_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, control);
 }
 
 static void msix_set_enable(struct pci_dev *dev, int enable)
 {
-	int pos;
 	u16 control;
 
-	pos = pci_find_capability(dev, PCI_CAP_ID_MSIX);
-	if (pos) {
-		pci_read_config_word(dev, pos + PCI_MSIX_FLAGS, &control);
-		control &= ~PCI_MSIX_FLAGS_ENABLE;
-		if (enable)
-			control |= PCI_MSIX_FLAGS_ENABLE;
-		pci_write_config_word(dev, pos + PCI_MSIX_FLAGS, control);
-	}
+	pci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &control);
+	control &= ~PCI_MSIX_FLAGS_ENABLE;
+	if (enable)
+		control |= PCI_MSIX_FLAGS_ENABLE;
+	pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, control);
 }
 
 static int (*get_owner)(struct pci_dev *dev);
@@ -239,13 +235,13 @@ static void msi_unmap_pirq(struct pci_de
 		evtchn_map_pirq(pirq, 0, nr);
 }
 
-static u64 find_table_base(struct pci_dev *dev, int pos)
+static u64 find_table_base(struct pci_dev *dev)
 {
 	u8 bar;
 	u32 reg;
 	unsigned long flags;
 
- 	pci_read_config_dword(dev, msix_table_offset_reg(pos), &reg);
+	pci_read_config_dword(dev, dev->msix_cap + PCI_MSIX_TABLE, &reg);
 	bar = reg & PCI_MSIX_FLAGS_BIRMASK;
 
 	flags = pci_resource_flags(dev, bar);
@@ -348,22 +344,21 @@ static void pci_intx_for_msi(struct pci_
 
 void pci_restore_msi_state(struct pci_dev *dev)
 {
-	int rc = -ENOSYS, pos = 0;
+	int rc = -ENOSYS;
 	u16 control = 0;
 
 	if (!dev->msi_enabled && !dev->msix_enabled)
 		return;
 
 	pci_intx_for_msi(dev, 0);
-	if (dev->msi_enabled) {
-		pos = pci_find_capability(dev, PCI_CAP_ID_MSI);
-		msi_set_enable(dev, pos, 0);
-	}
+	if (dev->msi_enabled)
+		msi_set_enable(dev, 0);
 	if (dev->msix_enabled) {
-		pos = pci_find_capability(dev, PCI_CAP_ID_MSIX);
-		pci_read_config_word(dev, pos + PCI_MSIX_FLAGS, &control);
+		pci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS,
+				     &control);
 		control |= PCI_MSIX_FLAGS_ENABLE | PCI_MSIX_FLAGS_MASKALL;
-		pci_write_config_word(dev, pos + PCI_MSIX_FLAGS, control);
+		pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS,
+				      control);
 	}
 
 	if (pci_seg_supported) {
@@ -391,7 +386,8 @@ void pci_restore_msi_state(struct pci_de
 
 	if (dev->msix_enabled) {
 	 	control &= ~PCI_MSIX_FLAGS_MASKALL;
- 		pci_write_config_word(dev, pos + PCI_MSIX_FLAGS, control);
+ 		pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS,
+				      control);
 	}
 }
 EXPORT_SYMBOL_GPL(pci_restore_msi_state);
@@ -546,10 +542,9 @@ out_unroll:
 static int msi_capability_init(struct pci_dev *dev, int nvec)
 {
 	struct msi_dev_list *dev_entry = get_msi_dev_pirq_list(dev);
-	int pos, pirq;
+	int pirq;
 
-	pos = pci_find_capability(dev, PCI_CAP_ID_MSI);
-	msi_set_enable(dev, pos, 0);	/* Disable MSI during set up */
+	msi_set_enable(dev, 0);	/* Disable MSI during set up */
 
 	pirq = msi_map_vector(dev, nvec, 0, dev_entry->owner);
 	if (pirq < 0)
@@ -558,7 +553,7 @@ static int msi_capability_init(struct pc
 
 	/* Set MSI enabled bits	 */
 	pci_intx_for_msi(dev, 0);
-	msi_set_enable(dev, pos, 1);
+	msi_set_enable(dev, 1);
 	dev->msi_enabled = 1;
 
 	dev->irq = dev_entry->e.pirq = pirq;
@@ -580,7 +575,7 @@ static int msix_capability_init(struct p
 				struct msix_entry *entries, int nvec)
 {
 	u64 table_base;
-	int pirq, i, j, mapped, pos;
+	int pirq, i, j, mapped;
 	u16 control;
 	struct msi_dev_list *msi_dev_entry = get_msi_dev_pirq_list(dev);
 	struct msi_pirq_entry *pirq_entry;
@@ -588,14 +583,13 @@ static int msix_capability_init(struct p
 	if (!msi_dev_entry)
 		return -ENOMEM;
 
-	pos = pci_find_capability(dev, PCI_CAP_ID_MSIX);
-	pci_read_config_word(dev, pos + PCI_MSIX_FLAGS, &control);
+	pci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &control);
 
 	/* Ensure MSI-X is disabled while it is set up */
 	control &= ~PCI_MSIX_FLAGS_ENABLE;
-	pci_write_config_word(dev, pos + PCI_MSIX_FLAGS, control);
+	pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, control);
 
-	table_base = find_table_base(dev, pos);
+	table_base = find_table_base(dev);
 	if (!table_base)
 		return -ENODEV;
 
@@ -605,7 +599,7 @@ static int msix_capability_init(struct p
 	 * interrupts coming in before they're fully set up.
 	 */
 	control |= PCI_MSIX_FLAGS_MASKALL | PCI_MSIX_FLAGS_ENABLE;
-	pci_write_config_word(dev, pos + PCI_MSIX_FLAGS, control);
+	pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, control);
 
 	for (i = 0; i < nvec; i++) {
 		mapped = 0;
@@ -655,7 +649,7 @@ static int msix_capability_init(struct p
 	populate_msi_sysfs(dev);
 
 	control &= ~PCI_MSIX_FLAGS_MASKALL;
-	pci_write_config_word(dev, pos + PCI_MSIX_FLAGS, control);
+	pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, control);
 
 	return 0;
 }
@@ -702,9 +696,6 @@ static int pci_msi_check_device(struct p
 	if (ret)
 		return ret;
 
-	if (!pci_find_capability(dev, type))
-		return -EINVAL;
-
 	return 0;
 }
 
@@ -723,14 +714,14 @@ static int pci_msi_check_device(struct p
  */
 int pci_enable_msi_block(struct pci_dev *dev, unsigned int nvec)
 {
-	int temp, status, pos, maxvec;
+	int temp, status, maxvec;
 	u16 msgctl;
 	struct msi_dev_list *msi_dev_entry = get_msi_dev_pirq_list(dev);
 
-	pos = pci_find_capability(dev, PCI_CAP_ID_MSI);
-	if (!pos)
+	if (!dev->msi_cap)
 		return -EINVAL;
-	pci_read_config_word(dev, pos + PCI_MSI_FLAGS, &msgctl);
+
+	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &msgctl);
 	if (msi_multi_vec_supported)
 		maxvec = 1 << ((msgctl & PCI_MSI_FLAGS_QMASK) >> 1);
 	else
@@ -783,14 +774,13 @@ EXPORT_SYMBOL(pci_enable_msi_block);
 
 int pci_enable_msi_block_auto(struct pci_dev *dev, unsigned int *maxvec)
 {
-	int ret, pos, nvec;
+	int ret, nvec;
 	u16 msgctl;
 
-	pos = pci_find_capability(dev, PCI_CAP_ID_MSI);
-	if (!pos)
+	if (!dev->msi_cap)
 		return -EINVAL;
 
-	pci_read_config_word(dev, pos + PCI_MSI_FLAGS, &msgctl);
+	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &msgctl);
 	ret = 1 << ((msgctl & PCI_MSI_FLAGS_QMASK) >> 1);
 
 	if (maxvec)
@@ -809,7 +799,7 @@ EXPORT_SYMBOL(pci_enable_msi_block_auto)
 
 void pci_msi_shutdown(struct pci_dev *dev)
 {
-	int pirq, pos;
+	int pirq;
 	struct msi_dev_list *msi_dev_entry = get_msi_dev_pirq_list(dev);
 
 	if (!pci_msi_enable || !dev || !dev->msi_enabled)
@@ -832,8 +822,7 @@ void pci_msi_shutdown(struct pci_dev *de
 
 	/* Disable MSI mode */
 	if (is_initial_xendomain()) {
-		pos = pci_find_capability(dev, PCI_CAP_ID_MSI);
-		msi_set_enable(dev, pos, 0);
+		msi_set_enable(dev, 0);
 		pci_intx_for_msi(dev, 1);
 	}
 	dev->msi_enabled = 0;
@@ -853,15 +842,13 @@ EXPORT_SYMBOL(pci_disable_msi);
  */
 int pci_msix_table_size(struct pci_dev *dev)
 {
-	int pos;
 	u16 control;
 
-	pos = pci_find_capability(dev, PCI_CAP_ID_MSIX);
-	if (!pos)
+	if (!dev->msix_cap)
 		return 0;
 
-	pci_read_config_word(dev, msi_control_reg(pos), &control);
-	return multi_msix_capable(control);
+	pci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &control);
+	return msix_table_size(control);
 }
 
 /**
@@ -885,7 +872,7 @@ int pci_enable_msix(struct pci_dev *dev,
 	int i, j, temp;
 	struct msi_dev_list *msi_dev_entry = get_msi_dev_pirq_list(dev);
 
-	if (!entries)
+	if (!entries || !dev->msix_cap)
 		return -EINVAL;
 
 	if (!is_initial_xendomain()) {
@@ -1052,7 +1039,6 @@ EXPORT_SYMBOL(pci_msi_enabled);
 
 void pci_msi_init_pci_dev(struct pci_dev *dev)
 {
-	int pos;
 	INIT_LIST_HEAD(&dev->msi_list);
 
 	/* Disable the msi hardware to avoid screaming interrupts
@@ -1063,12 +1049,15 @@ void pci_msi_init_pci_dev(struct pci_dev
 	 *   already enabled on purpose),
 	 * - unprivileged domains.
 	 */
-	if (!is_initial_xendomain()
-	    || ((dev->class >> 8) == PCI_CLASS_SYSTEM_IOMMU
-	        && dev->vendor == PCI_VENDOR_ID_AMD))
+	if (is_initial_xendomain()
+	    && (dev->class >> 8) == PCI_CLASS_SYSTEM_IOMMU
+	    && dev->vendor == PCI_VENDOR_ID_AMD)
 		return;
-	pos = pci_find_capability(dev, PCI_CAP_ID_MSI);
-	if (pos)
-		msi_set_enable(dev, pos, 0);
-	msix_set_enable(dev, 0);
+	dev->msi_cap = pci_find_capability(dev, PCI_CAP_ID_MSI);
+	if (dev->msi_cap && is_initial_xendomain())
+		msi_set_enable(dev, 0);
+
+	dev->msix_cap = pci_find_capability(dev, PCI_CAP_ID_MSIX);
+	if (dev->msix_cap && is_initial_xendomain())
+		msix_set_enable(dev, 0);
 }
--- a/drivers/platform/x86/Kconfig
+++ b/drivers/platform/x86/Kconfig
@@ -811,7 +811,7 @@ config INTEL_SMARTCONNECT
 
 config PVPANIC
 	tristate "pvpanic device support"
-	depends on ACPI
+	depends on ACPI && !XEN
 	---help---
 	  This driver provides support for the pvpanic device.  pvpanic is
 	  a paravirtualized device provided by QEMU; it lets a virtual machine
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@ -8682,9 +8682,6 @@ lpfc_sli4_set_affinity(struct lpfc_hba *
 	int max_phys_id, min_phys_id;
 	int num_io_channel, first_cpu, chan;
 	struct lpfc_vector_map_info *cpup;
-#ifdef CONFIG_X86
-	struct cpuinfo_x86 *cpuinfo;
-#endif
 	uint8_t chann[LPFC_FCP_IO_CHAN_MAX+1];
 
 	/* If there is no mapping, just return */
@@ -8705,8 +8702,8 @@ lpfc_sli4_set_affinity(struct lpfc_hba *
 	/* Update CPU map with physical id and core id of each CPU */
 	cpup = phba->sli4_hba.cpu_map;
 	for (cpu = 0; cpu < phba->sli4_hba.num_present_cpu; cpu++) {
-#ifdef CONFIG_X86
-		cpuinfo = &cpu_data(cpu);
+#if defined(CONFIG_X86) && !defined(CONFIG_XEN)
+		const struct cpuinfo_x86 *cpuinfo = &cpu_data(cpu);
 		cpup->phys_id = cpuinfo->phys_proc_id;
 		cpup->core_id = cpuinfo->cpu_core_id;
 #else
--- a/drivers/xen/balloon/balloon.c
+++ b/drivers/xen/balloon/balloon.c
@@ -43,6 +43,7 @@
 #include <linux/highmem.h>
 #include <linux/slab.h>
 #include <linux/mutex.h>
+#include <linux/seq_file.h>
 #include <xen/xen_proc.h>
 #include <asm/hypervisor.h>
 #include <xen/balloon.h>
@@ -60,10 +61,6 @@
 #include <xen/platform-compat.h>
 #endif
 
-#ifdef CONFIG_PROC_FS
-static struct proc_dir_entry *balloon_pde;
-#endif
-
 static DEFINE_MUTEX(balloon_mutex);
 
 /*
@@ -475,8 +472,8 @@ static int balloon_init_watcher(struct n
 }
 
 #ifdef CONFIG_PROC_FS
-static int balloon_write(struct file *file, const char __user *buffer,
-			 unsigned long count, void *data)
+static ssize_t balloon_write(struct file *file, const char __user *buffer,
+                           size_t count, loff_t *ppos)
 {
 	char memstring[64], *endchar;
 	unsigned long long target_bytes;
@@ -499,13 +496,9 @@ static int balloon_write(struct file *fi
 	return count;
 }
 
-static int balloon_read(char *page, char **start, off_t off,
-			int count, int *eof, void *data)
+static int balloon_show(struct seq_file *m, void *v)
 {
-	int len;
-
-	len = sprintf(
-		page,
+	return seq_printf(m,
 		"Current allocation: %8lu kB\n"
 		"Requested target:   %8lu kB\n"
 		"Low-mem balloon:    %8lu kB\n"
@@ -514,11 +507,20 @@ static int balloon_read(char *page, char
 		PAGES2KB(bs.current_pages), PAGES2KB(bs.target_pages), 
 		PAGES2KB(bs.balloon_low), PAGES2KB(bs.balloon_high),
 		PAGES2KB(bs.driver_pages));
+}
 
-
-	*eof = 1;
-	return len;
+static int balloon_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, balloon_show, PDE_DATA(inode));
 }
+
+static const struct file_operations balloon_fops = {
+	.open = balloon_open,
+	.llseek = seq_lseek,
+	.read = seq_read,
+	.write = balloon_write,
+	.release = single_release
+};
 #endif
 
 static struct notifier_block xenstore_notifier;
@@ -573,13 +575,11 @@ static int __init balloon_init(void)
 	bs.driver_pages  = 0UL;
 
 #ifdef CONFIG_PROC_FS
-	if ((balloon_pde = create_xen_proc_entry("balloon", 0644)) == NULL) {
+	if (!create_xen_proc_entry("balloon", S_IFREG|S_IRUGO|S_IWUSR,
+				   &balloon_fops, NULL)) {
 		WPRINTK("Unable to create /proc/xen/balloon.\n");
 		return -1;
 	}
-
-	balloon_pde->read_proc  = balloon_read;
-	balloon_pde->write_proc = balloon_write;
 #endif
 	balloon_sysfs_init();
 
--- a/drivers/xen/blkfront/blkfront.c
+++ b/drivers/xen/blkfront/blkfront.c
@@ -667,9 +667,13 @@ int blkif_open(struct block_device *bd,
 int blkif_release(struct inode *inode, struct file *filep)
 {
 	struct gendisk *disk = inode->i_bdev->bd_disk;
-#else
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
 int blkif_release(struct gendisk *disk, fmode_t mode)
 {
+#else
+void blkif_release(struct gendisk *disk, fmode_t mode)
+{
+#define return(n) return
 #endif
 	struct blkfront_info *info = disk->private_data;
 	struct xenbus_device *xbdev;
@@ -677,7 +681,7 @@ int blkif_release(struct gendisk *disk,
 
 	bdput(bd);
 	if (bd->bd_openers)
-		return 0;
+		return(0);
 
 	/*
 	 * Check if we have been instructed to close. We will have
@@ -702,7 +706,8 @@ int blkif_release(struct gendisk *disk,
 		kfree(info);
 	}
 
-	return 0;
+	return(0);
+#undef return
 }
 
 
--- a/drivers/xen/blkfront/block.h
+++ b/drivers/xen/blkfront/block.h
@@ -125,7 +125,11 @@ extern int blkif_ioctl(struct inode *ino
 		       unsigned command, unsigned long argument);
 #else
 extern int blkif_open(struct block_device *bdev, fmode_t mode);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
 extern int blkif_release(struct gendisk *disk, fmode_t mode);
+#else
+extern void blkif_release(struct gendisk *disk, fmode_t mode);
+#endif
 extern int blkif_ioctl(struct block_device *bdev, fmode_t mode,
 		       unsigned command, unsigned long argument);
 #endif
--- a/drivers/xen/blktap/blktap.c
+++ b/drivers/xen/blktap/blktap.c
@@ -277,7 +277,8 @@ static inline unsigned int OFFSET_TO_SEG
     } while(0)
 
 
-static char *blktap_devnode(struct device *dev, umode_t *mode)
+static char *blktap_devnode(struct device *dev, umode_t *mode,
+			    kuid_t *uid, kgid_t *gid)
 {
 	return kasprintf(GFP_KERNEL, "xen/blktap%u", MINOR(dev->devt));
 }
--- a/drivers/xen/blktap2-new/device.c
+++ b/drivers/xen/blktap2-new/device.c
@@ -28,7 +28,7 @@ blktap_device_open(struct block_device *
 	return 0;
 }
 
-static int
+static void
 blktap_device_release(struct gendisk *disk, fmode_t mode)
 {
 	struct blktap_device *tapdev = disk->private_data;
@@ -41,8 +41,6 @@ blktap_device_release(struct gendisk *di
 		set_bit(BLKTAP_DEVICE_CLOSED, &tap->dev_inuse);
 		blktap_ring_kick_user(tap);
 	}
-
-	return 0;
 }
 
 static int
--- a/drivers/xen/blktap2/device.c
+++ b/drivers/xen/blktap2/device.c
@@ -54,7 +54,7 @@ blktap_device_open(struct block_device *
 	return 0;
 }
 
-static int
+static void
 blktap_device_release(struct gendisk *disk, fmode_t mode)
 {
 	struct blktap_device *dev = disk->private_data;
@@ -63,8 +63,6 @@ blktap_device_release(struct gendisk *di
 	dev->users--;
 	if (test_bit(BLKTAP_SHUTDOWN_REQUESTED, &tap->dev_inuse))
 		blktap_device_destroy(tap);
-
-	return 0;
 }
 
 static int
--- a/drivers/xen/core/smpboot.c
+++ b/drivers/xen/core/smpboot.c
@@ -14,6 +14,7 @@
 #include <linux/notifier.h>
 #include <linux/cpu.h>
 #include <linux/percpu.h>
+#include <linux/tick.h>
 #include <asm/desc.h>
 #include <asm/pgalloc.h>
 #include <xen/evtchn.h>
@@ -198,7 +199,7 @@ static void __cpuinit cpu_bringup(void)
 static void __cpuinit cpu_bringup_and_idle(void)
 {
 	cpu_bringup();
-	cpu_idle();
+	cpu_startup_entry(CPUHP_ONLINE);
 }
 
 static void __cpuinit cpu_initialize_context(unsigned int cpu,
@@ -422,6 +423,7 @@ void __ref play_dead(void)
 	preempt_enable_no_resched();
 	VOID(HYPERVISOR_vcpu_op(VCPUOP_down, smp_processor_id(), NULL));
 	cpu_bringup();
+	tick_nohz_idle_enter();
 #else
 	BUG();
 #endif
--- a/drivers/xen/core/xen_proc.c
+++ b/drivers/xen/core/xen_proc.c
@@ -8,12 +8,13 @@ struct proc_dir_entry *
 #ifndef MODULE
 __init
 #endif
-create_xen_proc_entry(const char *name, mode_t mode)
+create_xen_proc_entry(const char *name, mode_t mode,
+		      const struct file_operations *fops, void *data)
 {
 	if ( xen_base == NULL )
 		if ( (xen_base = proc_mkdir("xen", NULL)) == NULL )
 			panic("Couldn't create /proc/xen");
-	return create_proc_entry(name, mode, xen_base);
+	return proc_create_data(name, mode, xen_base, fops, data);
 }
 
 #ifdef MODULE
--- a/drivers/xen/netback/netback.c
+++ b/drivers/xen/netback/netback.c
@@ -1759,6 +1759,8 @@ static void net_tx_action(unsigned long
 			continue;
 		}
 
+		skb_probe_transport_header(skb, 0);
+
 		dev->stats.rx_bytes += skb->len;
 		dev->stats.rx_packets++;
 
--- a/drivers/xen/privcmd/privcmd.c
+++ b/drivers/xen/privcmd/privcmd.c
@@ -8,6 +8,7 @@
 
 #include <linux/kernel.h>
 #include <linux/sched.h>
+#include <linux/seq_file.h>
 #include <linux/slab.h>
 #include <linux/string.h>
 #include <linux/errno.h>
@@ -24,9 +25,6 @@
 #include <xen/xen_proc.h>
 #include <xen/features.h>
 
-static struct proc_dir_entry *privcmd_intf;
-static struct proc_dir_entry *capabilities_intf;
-
 #ifndef HAVE_ARCH_PRIVCMD_MMAP
 static int enforce_singleshot_mapping_fn(pte_t *pte, struct page *pmd_page,
 					 unsigned long addr, void *data)
@@ -466,34 +464,40 @@ static const struct file_operations priv
 	.open = nonseekable_open,
 	.llseek = no_llseek,
 	.unlocked_ioctl = privcmd_ioctl,
-	.mmap = privcmd_mmap,
+	.mmap = privcmd_mmap
 };
 
-static int capabilities_read(char *page, char **start, off_t off,
-			     int count, int *eof, void *data)
+static int capabilities_show(struct seq_file *m, void *v)
 {
 	int len = 0;
-	*page = 0;
 
 	if (is_initial_xendomain())
-		len = sprintf( page, "control_d\n" );
+		len = seq_printf(m, "control_d\n");
 
-	*eof = 1;
 	return len;
 }
 
+static int capabilities_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, capabilities_show, PDE_DATA(inode));
+}
+
+static const struct file_operations capabilities_fops = {
+	.open = capabilities_open,
+	.llseek = seq_lseek,
+	.read = seq_read,
+	.release = single_release
+};
+
 static int __init privcmd_init(void)
 {
 	if (!is_running_on_xen())
 		return -ENODEV;
 
-	privcmd_intf = create_xen_proc_entry("privcmd", 0400);
-	if (privcmd_intf != NULL)
-		privcmd_intf->proc_fops = &privcmd_file_ops;
-
-	capabilities_intf = create_xen_proc_entry("capabilities", 0400 );
-	if (capabilities_intf != NULL)
-		capabilities_intf->read_proc = capabilities_read;
+	create_xen_proc_entry("privcmd", S_IFREG|S_IRUSR,
+			      &privcmd_file_ops, NULL);
+	create_xen_proc_entry("capabilities", S_IFREG|S_IRUGO,
+			      &capabilities_fops, NULL);
 
 	return 0;
 }
--- a/drivers/xen/xenbus/xenbus_client.c
+++ b/drivers/xen/xenbus/xenbus_client.c
@@ -516,7 +516,7 @@ static int xenbus_map_ring_valloc_hvm(st
 
 	err = xenbus_map_ring(dev, gnt_ref, &node->handle, addr);
 	if (err)
-		goto out_err;
+		goto out_err_free_ballooned_pages;
 
 	spin_lock(&xenbus_valloc_lock);
 	list_add(&node->next, &xenbus_valloc_pages);
@@ -525,8 +525,9 @@ static int xenbus_map_ring_valloc_hvm(st
 	*vaddr = addr;
 	return 0;
 
- out_err:
+ out_err_free_ballooned_pages:
 	free_xenballooned_pages(1, &node->page);
+ out_err:
 	kfree(node);
 	return err;
 }
--- a/drivers/xen/xenbus/xenbus_comms.h
+++ b/drivers/xen/xenbus/xenbus_comms.h
@@ -46,6 +46,7 @@ int xb_wait_for_data_to_read_kgraft(void
 int xs_input_avail(void);
 extern struct xenstore_domain_interface *xen_store_interface;
 extern int xen_store_evtchn;
+extern enum xenstore_init xen_store_domain_type;
 
 extern const struct file_operations xen_xenbus_fops;
 
--- a/drivers/xen/xenbus/xenbus_dev.c
+++ b/drivers/xen/xenbus/xenbus_dev.c
@@ -90,8 +90,6 @@ struct xenbus_dev_data {
 	struct mutex reply_mutex;
 };
 
-static struct proc_dir_entry *xenbus_dev_intf;
-
 static ssize_t xenbus_dev_read(struct file *filp,
 			       char __user *ubuf,
 			       size_t len, loff_t *ppos)
@@ -505,9 +503,9 @@ __init
 #endif
 xenbus_dev_init(void)
 {
-	xenbus_dev_intf = create_xen_proc_entry("xenbus", 0400);
-	if (xenbus_dev_intf)
-		xenbus_dev_intf->proc_fops = &xenbus_dev_file_ops;
+	if (!create_xen_proc_entry("xenbus", S_IFREG|S_IRUSR,
+				   &xenbus_dev_file_ops, NULL))
+		return -ENOMEM;
 
 	return 0;
 }
--- a/drivers/xen/xenbus/xenbus_probe.c
+++ b/drivers/xen/xenbus/xenbus_probe.c
@@ -43,7 +43,6 @@
 #include <linux/fcntl.h>
 #include <linux/mm.h>
 #include <linux/sched.h>
-#include <linux/proc_fs.h>
 #include <linux/notifier.h>
 #include <linux/mutex.h>
 #include <linux/io.h>
@@ -55,13 +54,15 @@
 #if defined(CONFIG_XEN) || defined(MODULE)
 #include <asm/hypervisor.h>
 #include <xen/xenbus.h>
-#include <xen/xen_proc.h>
 #include <xen/evtchn.h>
 #include <xen/features.h>
 #include <xen/gnttab.h>
 
 #define PARAVIRT_EXPORT_SYMBOL(sym) __typeof__(sym) sym
 #else
+#if defined(CONFIG_XEN_COMPAT_XENFS) && !defined(MODULE)
+#include <linux/proc_fs.h>
+#endif
 #include <asm/xen/hypervisor.h>
 
 #include <xen/xen.h>
@@ -89,6 +90,11 @@ EXPORT_SYMBOL_GPL(xen_store_evtchn);
 struct xenstore_domain_interface *xen_store_interface;
 EXPORT_SYMBOL_GPL(xen_store_interface);
 
+#if !defined(CONFIG_XEN) && !defined(MODULE)
+enum xenstore_init xen_store_domain_type;
+EXPORT_SYMBOL_GPL(xen_store_domain_type);
+#endif
+
 static unsigned long xen_store_mfn;
 
 extern struct mutex xenwatch_mutex;
@@ -1105,9 +1111,8 @@ device_initcall(xenbus_probe_initcall);
 
 #ifdef CONFIG_XEN_PRIVILEGED_GUEST
 #ifdef CONFIG_PROC_FS
-static struct file_operations xsd_kva_fops;
-static struct proc_dir_entry *xsd_kva_intf;
-static struct proc_dir_entry *xsd_port_intf;
+#include <linux/seq_file.h>
+#include <xen/xen_proc.h>
 
 static int xsd_kva_mmap(struct file *file, struct vm_area_struct *vma)
 {
@@ -1145,25 +1150,40 @@ static int xsd_kva_mmap(struct file *fil
 	return 0;
 }
 
-static int xsd_kva_read(char *page, char **start, off_t off,
-			int count, int *eof, void *data)
+static int xsd_kva_show(struct seq_file *m, void *v)
 {
-	int len;
+	return seq_printf(m, "0x%p", xen_store_interface);
+}
 
-	len  = sprintf(page, "0x%p", xen_store_interface);
-	*eof = 1;
-	return len;
+static int xsd_kva_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, xsd_kva_show, PDE_DATA(inode));
 }
 
-static int xsd_port_read(char *page, char **start, off_t off,
-			 int count, int *eof, void *data)
+static const struct file_operations xsd_kva_fops = {
+	.open = xsd_kva_open,
+	.llseek = seq_lseek,
+	.read = seq_read,
+	.mmap = xsd_kva_mmap,
+	.release = single_release
+};
+
+static int xsd_port_show(struct seq_file *m, void *v)
 {
-	int len;
+	return seq_printf(m, "%d", xen_store_evtchn);
+}
 
-	len  = sprintf(page, "%d", xen_store_evtchn);
-	*eof = 1;
-	return len;
+static int xsd_port_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, xsd_port_show, PDE_DATA(inode));
 }
+
+static const struct file_operations xsd_port_fops = {
+	.open = xsd_port_open,
+	.llseek = seq_lseek,
+	.read = seq_read,
+	.release = single_release
+};
 #endif
 
 #ifdef CONFIG_XEN_XENBUS_DEV
@@ -1255,12 +1275,6 @@ static int __init xenstored_local_init(v
 	return err;
 }
 
-enum xenstore_init {
-	UNKNOWN,
-	PV,
-	HVM,
-	LOCAL,
-};
 #ifndef MODULE
 static int __init
 #else
@@ -1270,8 +1284,8 @@ xenbus_init(void)
 {
 	int err = 0;
 #if !defined(CONFIG_XEN) && !defined(MODULE)
-	enum xenstore_init usage = UNKNOWN;
 	uint64_t v = 0;
+	xen_store_domain_type = XS_UNKNOWN;
 #endif
 
 	DPRINTK("");
@@ -1297,17 +1311,10 @@ xenbus_init(void)
 
 #if defined(CONFIG_PROC_FS) && defined(CONFIG_XEN_PRIVILEGED_GUEST)
 		/* And finally publish the above info in /proc/xen */
-		xsd_kva_intf = create_xen_proc_entry("xsd_kva", 0600);
-		if (xsd_kva_intf) {
-			memcpy(&xsd_kva_fops, xsd_kva_intf->proc_fops,
-			       sizeof(xsd_kva_fops));
-			xsd_kva_fops.mmap = xsd_kva_mmap;
-			xsd_kva_intf->proc_fops = &xsd_kva_fops;
-			xsd_kva_intf->read_proc = xsd_kva_read;
-		}
-		xsd_port_intf = create_xen_proc_entry("xsd_port", 0400);
-		if (xsd_port_intf)
-			xsd_port_intf->read_proc = xsd_port_read;
+		create_xen_proc_entry("xsd_kva", S_IFREG|S_IRUSR|S_IWUSR,
+				      &xsd_kva_fops, NULL);
+		create_xen_proc_entry("xsd_port", S_IFREG|S_IRUSR,
+				      &xsd_port_fops, NULL);
 #endif
 		xen_store_interface = mfn_to_virt(xen_store_mfn);
 	} else {
@@ -1343,29 +1350,29 @@ xenbus_init(void)
 	xenbus_ring_ops_init();
 
 	if (xen_pv_domain())
-		usage = PV;
+		xen_store_domain_type = XS_PV;
 	if (xen_hvm_domain())
-		usage = HVM;
+		xen_store_domain_type = XS_HVM;
 	if (xen_hvm_domain() && xen_initial_domain())
-		usage = LOCAL;
+		xen_store_domain_type = XS_LOCAL;
 	if (xen_pv_domain() && !xen_start_info->store_evtchn)
-		usage = LOCAL;
+		xen_store_domain_type = XS_LOCAL;
 	if (xen_pv_domain() && xen_start_info->store_evtchn)
 		atomic_set(&xenbus_xsd_state, XENBUS_XSD_FOREIGN_READY);
 
-	switch (usage) {
-	case LOCAL:
+	switch (xen_store_domain_type) {
+	case XS_LOCAL:
 		err = xenstored_local_init();
 		if (err)
 			goto out_error;
 		xen_store_interface = mfn_to_virt(xen_store_mfn);
 		break;
-	case PV:
+	case XS_PV:
 		xen_store_evtchn = xen_start_info->store_evtchn;
 		xen_store_mfn = xen_start_info->store_mfn;
 		xen_store_interface = mfn_to_virt(xen_store_mfn);
 		break;
-	case HVM:
+	case XS_HVM:
 		err = hvm_get_parameter(HVM_PARAM_STORE_EVTCHN, &v);
 		if (err)
 			goto out_error;
--- a/drivers/xen/xenbus/xenbus_probe.h
+++ b/drivers/xen/xenbus/xenbus_probe.h
@@ -77,6 +77,13 @@ struct xen_bus_type {
 	struct bus_type bus;
 };
 
+enum xenstore_init {
+	XS_UNKNOWN,
+	XS_PV,
+	XS_HVM,
+	XS_LOCAL,
+};
+
 extern struct device_attribute xenbus_dev_attrs[];
 
 extern int xenbus_match(struct device *_dev, struct device_driver *_drv);
--- a/fs/aio.c
+++ b/fs/aio.c
@@ -1162,8 +1162,8 @@ EXPORT_SYMBOL(aio_complete);
 /* aio_read_events
  *	Pull an event off of the ioctx's event ring.  Returns the number of
  *	events fetched
- *	If ent parameter is 0, just returns the number of events that would
- *	be fetched.
+ *	If event parameter is NULL, just returns the number of events that
+ *	would be fetched.
  */
 static long aio_read_events_ring(struct kioctx *ctx,
 				 struct io_event __user *event, long nr)
@@ -1208,10 +1208,12 @@ static long aio_read_events_ring(struct
 		avail = min_t(long, avail, AIO_EVENTS_PER_PAGE -
 			    ((head + AIO_EVENTS_OFFSET) % AIO_EVENTS_PER_PAGE));
 
+#ifdef CONFIG_EPOLL
 		if (!event) { /* only need to know availability */
 			ret = avail;
 			goto out;
 		}
+#endif
 
 		pos = head + AIO_EVENTS_OFFSET;
 		page = ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE];
@@ -1268,6 +1270,11 @@ static long read_events(struct kioctx *c
 	ktime_t until = { .tv64 = KTIME_MAX };
 	long ret = 0;
 
+#ifdef CONFIG_EPOLL
+	if (!event && nr)
+		return -EFAULT;
+#endif
+
 	if (timeout) {
 		struct timespec	ts;
 
@@ -1320,15 +1327,14 @@ static unsigned int aio_queue_fd_poll(st
 	struct kioctx *ioctx = file->private_data;
 
 	if (ioctx) {
-
-		spin_lock_irq(&ioctx->ctx_lock);
 		/* Insert inside our poll wait queue */
+		spin_lock_irq(&ioctx->ctx_lock);
 		poll_wait(file, &ioctx->poll_wait, wait);
+		spin_unlock_irq(&ioctx->ctx_lock);
 
 		/* Check our condition */
-		if (aio_read_evt(ioctx, 0))
+		if (aio_read_events_ring(ioctx, NULL, 1))
 			pollflags = POLLIN | POLLRDNORM;
-		spin_unlock_irq(&ioctx->ctx_lock);
 	}
 
 	return pollflags;
--- a/include/xen/net-util.h
+++ b/include/xen/net-util.h
@@ -6,13 +6,11 @@
 #include <linux/tcp.h>
 #include <linux/udp.h>
 #include <net/ip.h>
-#include <net/flow_keys.h>
 
 static inline int skb_checksum_setup(struct sk_buff *skb,
 				     unsigned long *fixup_counter)
 {
 	struct iphdr *iph = (void *)skb->data;
-	unsigned char *th;
 	__be16 *csum = NULL;
 	int err = -EPROTO;
 
@@ -35,22 +33,20 @@ static inline int skb_checksum_setup(str
 	if (skb->protocol != htons(ETH_P_IP))
 		goto out;
 
-	th = skb->data + 4 * iph->ihl;
-	if (th >= skb_tail_pointer(skb))
-		goto out;
-
-	skb_set_transport_header(skb, 4 * iph->ihl);
-	skb->csum_start = th - skb->head;
 	switch (iph->protocol) {
 	case IPPROTO_TCP:
-		skb->csum_offset = offsetof(struct tcphdr, check);
+		if (!skb_partial_csum_set(skb, 4 * iph->ihl,
+					  offsetof(struct tcphdr, check)))
+			goto out;
 		if (csum)
-			csum = &((struct tcphdr *)th)->check;
+			csum = &tcp_hdr(skb)->check;
 		break;
 	case IPPROTO_UDP:
-		skb->csum_offset = offsetof(struct udphdr, check);
+		if (!skb_partial_csum_set(skb, 4 * iph->ihl,
+					  offsetof(struct udphdr, check)))
+			goto out;
 		if (csum)
-			csum = &((struct udphdr *)th)->check;
+			csum = &udp_hdr(skb)->check;
 		break;
 	default:
 		net_err_ratelimited("Attempting to checksum a non-TCP/UDP packet,"
@@ -59,24 +55,12 @@ static inline int skb_checksum_setup(str
 		goto out;
 	}
 
-	if ((th + skb->csum_offset + sizeof(*csum)) > skb_tail_pointer(skb))
-		goto out;
-
-	if (csum) {
+	if (csum)
 		*csum = ~csum_tcpudp_magic(iph->saddr, iph->daddr,
 					   skb->len - iph->ihl*4,
 					   iph->protocol, 0);
-		skb->ip_summed = CHECKSUM_PARTIAL;
-	}
-
-	if (!skb_transport_header_was_set(skb)) {
-		struct flow_keys keys;
 
-		if (skb_flow_dissect(skb, &keys))
-			skb_set_transport_header(skb, keys.thoff);
-		else
-			skb_reset_transport_header(skb);
-	}
+	skb_probe_transport_header(skb, 0);
 
 	err = 0;
 out:
--- a/include/xen/xen_proc.h
+++ b/include/xen/xen_proc.h
@@ -4,9 +4,9 @@
 
 #include <linux/proc_fs.h>
 
-extern struct proc_dir_entry *create_xen_proc_entry(
-	const char *name, mode_t mode);
-extern void remove_xen_proc_entry(
-	const char *name);
+struct proc_dir_entry *create_xen_proc_entry(const char *, mode_t,
+					     const struct file_operations *,
+					     void *data);
+void remove_xen_proc_entry(const char *);
 
 #endif /* __ASM_XEN_PROC_H__ */
--- a/include/xen/xenbus.h
+++ b/include/xen/xenbus.h
@@ -85,6 +85,9 @@ struct xenbus_device {
 	struct device dev;
 	enum xenbus_state state;
 	struct completion down;
+#if !defined(CONFIG_XEN) && !defined(HAVE_XEN_PLATFORM_COMPAT_H)
+	struct work_struct work;
+#endif
 };
 
 static inline struct xenbus_device *to_xenbus_device(struct device *dev)
--- a/kernel/locking/mutex.c
+++ b/kernel/locking/mutex.c
@@ -184,7 +184,8 @@ static inline int mutex_can_spin_on_owne
 	rcu_read_lock();
 	owner = ACCESS_ONCE(lock->owner);
 	if (owner)
-		retval = owner->on_cpu;
+		retval = owner->on_cpu &&
+			 arch_cpu_is_running(task_thread_info(owner)->cpu);
 	rcu_read_unlock();
 	/*
 	 * if lock->owner is not set, the mutex owner may have just acquired
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -563,6 +563,10 @@ int timekeeping_inject_offset(struct tim
 	tk_xtime_add(tk, ts);
 	tk_set_wall_to_mono(tk, timespec_sub(tk->wall_to_monotonic, *ts));
 
+#ifdef CONFIG_XEN_PRIVILEGED_GUEST
+	xen_update_wallclock(&tmp);
+#endif
+
 error: /* even if we error out, we forwarded the time, so call update */
 	timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);
 
